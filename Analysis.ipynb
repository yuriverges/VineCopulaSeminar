{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sci\n",
    "import arch\n",
    "import pyvinecopulib as pv\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from SourceCodes.invariance_analysis import quick_invariance_analysis\n",
    "from SourceCodes.garch_aux_methods import zero_mean_garch_1_1_scenario\n",
    "\n",
    "\n",
    "columns_mappings = {'ES1 Index': 'SP500',\n",
    "                    'NQ1 Index': 'Nasdaq100',\n",
    "                    'VG1 Index': 'Euro Stox50',\n",
    "                    'BZ1 Index': 'Ibovespa',\n",
    "                    'TY1 Comdty':'10-Year Treasury',\n",
    "                    'RX1 Comdty': 'Euro Bund',\n",
    "                    'EC1 Curncy': 'EUR/USD',\n",
    "                    'BP1 Curncy': 'GBP/USD',\n",
    "                    'UC1 Curncy': 'USD/BRL',\n",
    "                    'CL1 Comdty': 'WTI',\n",
    "                    'CO1 Comdty': 'Brent'}\n",
    "\n",
    "selected_securities = ['SP500', 'Nasdaq100', 'Ibovespa', '10-Year Treasury','USD/BRL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "df = pd.read_csv(\"Data/data.csv\", index_col=\"Dates\", date_parser = pd.to_datetime).rename(columns=columns_mappings)[selected_securities]\n",
    "df.sort_index(inplace=True)\n",
    "returns_scale_factor = 100\n",
    "df_log_ret = np.log(df).diff() * returns_scale_factor\n",
    "n = df.shape[0]\n",
    "n_os = 252\n",
    "n_scenarios = 5000\n",
    "seeds = [int(x) for x in np.ones(len(selected_securities)).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Quick Invariance Analysis\n",
    "for item in df_log_ret.columns:\n",
    "    fig, axs = quick_invariance_analysis(df_log_ret[item].iloc[:-n_os], n_chunks=3, nbins=30)\n",
    "    fig.set_size_inches(20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a GARCH(1,1) Model and\n",
    "model_list = []\n",
    "\n",
    "for k, item in enumerate(df_log_ret):\n",
    "    mdl = list()\n",
    "    mdl.append(arch.arch_model(df_log_ret.iloc[1:-n_os, k] , mean=\"Zero\", vol=\"GARCH\", p=1, q=1, dist=\"normal\"))\n",
    "    mdl[0].constraints()\n",
    "    mdl.append(mdl[0].fit())\n",
    "    model_list.append(mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - Univariate Fitting\n",
    "fig = model_list[0][1].plot(scale=252)\n",
    "fig.set_size_inches(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the matrix of residuals and scatter plotting\n",
    "for k, item in enumerate(model_list):\n",
    "    if k == 0:\n",
    "        df_log_resid = pd.DataFrame(item[1].std_resid)\n",
    "        df_log_resid.rename(columns={df_log_resid.columns[-1]:item[0].y.name}, inplace=True)\n",
    "    else:\n",
    "        df_log_resid = pd.concat([df_log_resid, pd.DataFrame(item[1].std_resid)], axis=1)\n",
    "        df_log_resid.rename(columns={df_log_resid.columns[-1]:item[0].y.name}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Quick Invariance Analysis - Of residuals\n",
    "for item in df_log_resid.columns:\n",
    "    fig, axs = quick_invariance_analysis(df_log_resid[item], n_chunks=3, nbins=30)\n",
    "    fig.set_size_inches(20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the pseudo-observations - Estimated Probability Integral Transform\n",
    "for k, item in enumerate(model_list):\n",
    "    if k == 0:\n",
    "        df_log_resid_pseudo_observations = pd.DataFrame(sci.stats.norm.cdf(df_log_resid.iloc[:, k]))\n",
    "    else:\n",
    "        df_log_resid_pseudo_observations = pd.concat([df_log_resid_pseudo_observations, pd.Series(sci.stats.norm.cdf(df_log_resid.iloc[:, k]))], axis=1, ignore_index=True)\n",
    "\n",
    "df_log_resid_pseudo_observations.columns = df_log_resid.columns\n",
    "df_log_resid_pseudo_observations.columns = df_log_resid.columns\n",
    "df_log_resid_pseudo_observations.index=df_log_resid.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical Copula Plotting\n",
    "fig1 =  px.scatter_matrix(data_frame=df_log_resid_pseudo_observations,\n",
    "                          dimensions=df_log_resid_pseudo_observations.columns,\n",
    "                          height=2000, width=2000)\n",
    "fig1.update_traces(marker=dict(size=4, line=dict(width=1)), opacity=0.6, showlegend=False)\n",
    "#fig1.update_traces(diagonal_visible=False)\n",
    "fig1.update_layout(plot_bgcolor = \"#ffebe3\", colorway = [\"#ff774a\"], title = \"Empirical Copula\")\n",
    "fig1.update_layout({\"yaxis\"+str(i+1): dict(range = [0, 1]) for i in range(1, len(df.columns))})\n",
    "fig1.update_layout({\"xaxis\"+str(i+1): dict(range = [0, 1]) for i in range(1, len(df.columns))})\n",
    "fig1.update_xaxes(visible=True, showgrid=True)\n",
    "fig1.update_yaxes(visible=True, showgrid=True)\n",
    "fig1.write_html(\"HTML/file.html\")\n",
    "#fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta Distribution - Distribution which is constructed by an arbitrary copula and arbitrary marginal distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Vine - Structure\n",
    "copVine = pv.Vinecop(d = df_log_resid_pseudo_observations.columns.shape[0])\n",
    "\n",
    "# Selecting Most Appropriate Model Given pseudo-observations\n",
    "copVine.select(data=df_log_resid_pseudo_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(copVine.str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AIC: {copVine.aic()}\")\n",
    "print(f\"BIC: {copVine.bic()}\")\n",
    "print(f\"loglik: {copVine.loglik()}\")\n",
    "print(f\"Modified Bayesian Information Criteria: {copVine.mbicv()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating U - Given Vine - Empirical X Simulated Copulas\n",
    "n_sim = 10000\n",
    "u_sim = pd.DataFrame(copVine.simulate(n_sim, seeds=seeds), columns=df_log_resid_pseudo_observations.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting simulation results given Vine\n",
    "fig2 =  px.scatter_matrix(data_frame=u_sim,\n",
    "                          dimensions=u_sim.columns,\n",
    "                          height=2000, width=2000)\n",
    "fig2.update_traces(marker=dict(size=4, line=dict(width=1)), opacity=0.6, showlegend=False)\n",
    "fig2.update_layout(plot_bgcolor = \"#ffebe3\", colorway = [\"#ff774a\"], title = \"Simulated Data - Given Vine Structure\")\n",
    "fig2.update_layout({\"yaxis\"+str(i+1): dict(range = [0, 1]) for i in range(1, len(df.columns))})\n",
    "fig2.update_layout({\"xaxis\"+str(i+1): dict(range = [0, 1]) for i in range(1, len(df.columns))})\n",
    "fig2.update_xaxes(visible=True, showgrid=True)\n",
    "fig2.update_yaxes(visible=True, showgrid=True)\n",
    "fig2.write_html(\"HTML/simulated.html\")\n",
    "#fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario Simulation - Vine Copula\n",
    "scenarios_copula = list()\n",
    "\n",
    "for k in range(n_scenarios):\n",
    "    scenario_copula = dict()\n",
    "    scenario_copula['u_sim'] = pd.DataFrame(copVine.simulate(n_os), columns=df_log_resid_pseudo_observations.columns)\n",
    "\n",
    "    sim_given_models_copula = []\n",
    "    for d, model in enumerate(model_list):\n",
    "        # Filterting data - Given Models + Simulated Residuals\n",
    "        sim_given_models_copula.append(zero_mean_garch_1_1_scenario(sci.stats.norm.ppf(scenario_copula['u_sim'].iloc[:, d]), model[1].conditional_volatility,\n",
    "                                                             model[1].resid, w=model[1].params.omega, alpha=model[1].params['alpha[1]'],\n",
    "                                                             beta=model[1].params['beta[1]']))\n",
    "        # Return projection for the proposed horizon\n",
    "        if d == 0:\n",
    "            projected_returns_simulation_copula = pd.DataFrame(pd.Series(np.exp(np.cumsum(sim_given_models_copula[d][0] / returns_scale_factor)) - 1))\n",
    "        else:\n",
    "            projected_returns_simulation_copula = pd.concat([projected_returns_simulation_copula, pd.Series(np.exp(np.cumsum(sim_given_models_copula[d][0] / returns_scale_factor)) - 1)], axis=1, ignore_index=True)\n",
    "\n",
    "    projected_returns_simulation_copula.index = df_log_ret.index[-n_os:]\n",
    "    projected_returns_simulation_copula.columns = selected_securities\n",
    "\n",
    "    scenario_copula[\"sim_given_models\"] = sim_given_models_copula\n",
    "    scenario_copula[\"projections\"] = projected_returns_simulation_copula\n",
    "\n",
    "    scenarios_copula.append(scenario_copula)\n",
    "\n",
    "# Scenarios Projections\n",
    "scenario_projections_copula = np.zeros((n_scenarios, len(selected_securities)))\n",
    "\n",
    "for k, scenario in enumerate(scenarios_copula):\n",
    "    scenario_projections_copula[k, :] = scenario[\"projections\"].iloc[-1, :].to_numpy()\n",
    "\n",
    "scenario_projections_copula = pd.DataFrame(scenario_projections_copula, columns = selected_securities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ledoit and Wolf - Shrinked Gaussian Covariance Estimate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.covariance import ShrunkCovariance\n",
    "\n",
    "n_split = np.floor(df_log_resid.shape[0] / 2).astype(int)\n",
    "X_train = df_log_resid.iloc[0:n_split, :]\n",
    "X_test = df_log_resid.iloc[n_split:, :]\n",
    "\n",
    "shrinkages = np.logspace(-2, 0, 30)\n",
    "\n",
    "# GridSearch for an optimal shrinkage coefficient\n",
    "tuned_parameters = [{\"shrinkage\": shrinkages}]\n",
    "cv = GridSearchCV(ShrunkCovariance(), tuned_parameters)\n",
    "cv.fit(X_train)\n",
    "\n",
    "# Ledoit-Wolf optimal shrinkage coefficient estimate\n",
    "lw = LedoitWolf()\n",
    "lw_fit = lw.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario Simulation - Ledoit and Wolf\n",
    "scenarios_ld = list()\n",
    "\n",
    "for k in range(n_scenarios):\n",
    "    scenario_ld = dict()\n",
    "    scenario_ld['std_resid_sim'] = pd.DataFrame(np.random.multivariate_normal(mean=np.zeros(shape=(len(selected_securities))), cov=lw_fit.covariance_, size=n_os), columns=selected_securities)\n",
    "\n",
    "    sim_given_models_ld = []\n",
    "    for d, model in enumerate(model_list):\n",
    "        # Filterting data - Given Models + Simulated Residuals\n",
    "        sim_given_models_ld.append(zero_mean_garch_1_1_scenario(scenario_ld['std_resid_sim'].iloc[:, d].to_numpy(), model[1].conditional_volatility,\n",
    "                                                             model[1].resid, w=model[1].params.omega, alpha=model[1].params['alpha[1]'],\n",
    "                                                             beta=model[1].params['beta[1]']))\n",
    "        # Return projection for the proposed horizon\n",
    "        if d == 0:\n",
    "            projected_returns_simulation_ld = pd.DataFrame(pd.Series(np.exp(np.cumsum(sim_given_models_ld[d][0] / returns_scale_factor)) - 1))\n",
    "        else:\n",
    "            projected_returns_simulation_ld = pd.concat([projected_returns_simulation_ld, pd.Series(np.exp(np.cumsum(sim_given_models_ld[d][0] / returns_scale_factor)) - 1)], axis=1, ignore_index=True)\n",
    "\n",
    "    projected_returns_simulation_ld.index = df_log_ret.index[-n_os:]\n",
    "    projected_returns_simulation_ld.columns = selected_securities\n",
    "\n",
    "    scenario_ld[\"sim_given_models\"] = sim_given_models_ld\n",
    "    scenario_ld[\"projections\"] = projected_returns_simulation_ld\n",
    "\n",
    "    scenarios_ld.append(scenario_ld)\n",
    "\n",
    "# Scenarios Projections\n",
    "scenario_projections_ld = np.zeros((n_scenarios, len(selected_securities)))\n",
    "\n",
    "for k, scenario in enumerate(scenarios_ld):\n",
    "    scenario_projections_ld[k, :] = scenario[\"projections\"].iloc[-1, :].to_numpy()\n",
    "\n",
    "scenario_projections_ld = pd.DataFrame(scenario_projections_ld, columns = selected_securities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatilities - Vine Copula\n",
    "print(scenario_projections_copula.std())\n",
    "\n",
    "# Correlation - Vine Copula\n",
    "print(scenario_projections_copula.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatilities - Ledoit and Wolf\n",
    "print(scenario_projections_ld.std())\n",
    "\n",
    "# Correlation Ledoit and Wolf\n",
    "print(scenario_projections_ld.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Risk Budget\n",
    "\n",
    "Introduction to Risk Parity and Budgeting - Thierry Roncalli\n",
    "See http://www.thierry-roncalli.com/RiskParityBook.html\n",
    "\n",
    "\n",
    "#### Idea: a target in terms of risk contribution to each security in a portfolio\n",
    "\n",
    "A Risk measure $\\mathcal{R}$ is said coherent Artznet et al(1999) if it satisfies the following properties:\n",
    "- Homogeneity: $\\mathcal{R} \\left(\\lambda x\\right) = \\lambda\\mathcal{R}\\left(x\\right)$\n",
    "- Subadditivity(Risk of 2 portfolios smaller compared to the risk of two separate portfolios): $\\mathcal{R}\\left(x_1 + x_2\\right) \\leq \\mathcal{R}\\left(x_1\\right) \\mathcal{R}\\left(x_2\\right) $\n",
    "- Monotonicity(Order relation): $x_1 < x_2$, then $\\mathcal{R}\\left(x_1\\right) > \\mathcal{R}\\left(x_2\\right)$\n",
    "- Translation Invariance(Add cash amount reduce risk): if $m \\in \\mathbb{R}$, then $\\mathcal{R}\\left(x + m\\right) = \\mathcal{R}\\left(x\\right) - m$\n",
    "\n",
    "\n",
    "Follmer and Shied (2002) - Proposed to substitute Subadditivity and Homogeneity by a weaker condtion (diversification should not increase risk):\n",
    "- Convexity Property: $\\mathcal{R} \\left(\\lambda x_1 + \\left(1-\\lambda\\right) x_2 \\right) \\leq \\lambda\\mathcal{R}\\left(x_1\\right) + \\left(1 - \\lambda\\right)\\mathcal{R}\\left(x_1\\right)$\n",
    "\n",
    "\n",
    "By defining the loss as\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{L}\\left(x\\right) = -\\mathcal{R}\\left(x\\right)\n",
    "\\end{align}\n",
    "\n",
    "The idea of this problem is to set the targeted risk contribution depending on the risk measure selected through the \"Euler Allocation Principle\"\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{R}\\left(x\\right) = x'\\nabla \\mathcal{R\\left(x\\right)}\n",
    "\\end{align}\n",
    "\n",
    "A risk measure $\\mathcal{R}$ satisfying the properties listed above satisfies the principle(see Roncalli). For example, one can choose the volatility of the portfolio as a risk measure and target the contribution of each asset class in the aggregate portfolio.\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{R}\\left(x\\right) = \\sqrt{x' \\Sigma x}\n",
    "\\end{align}\n",
    "\n",
    "Then, each component of a portfolio could contribute marginally to the whole through the following sensitivities\n",
    "\n",
    "\\begin{align}\n",
    " \\frac{\\partial \\mathcal{R}\\left(x\\right)}{\\partial x} = \\frac{x'\\Sigma}{\\left(x'\\Sigma x\\right)^{\\frac{1}{2}}}\n",
    "\\end{align}\n",
    "\n",
    "Since the scenario modelling implemented in this work have adverse events and other multivariate characteristics being treated, an exercise constraining for the V@R will be implemented for analysis purposes\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{R}\\left(x\\right) = VaR_{\\alpha}\\left(x\\right) = inf\\{l: \\mathbb{P}\\{\\mathcal{L}\\left(x\\right) \\leq l\\} \\geq \\alpha\\}\n",
    "\\end{align}\n",
    "\n",
    "### Optimization Problem\n",
    "\n",
    "The Risk Budgeting portfolio will have as solution the one of the following non-linear problem\n",
    "\n",
    "\\begin{align}\n",
    "    x^* = \\{x \\in \\left[0, 1\\right]^n : \\sum_{i=1}^{n}x_i = 1, \\partial_{x_i} \\mathcal{R}\\left(x\\right) = b_i\\mathcal{R}\\left(x\\right), b \\in \\left]0,1\\right]^n, \\sum_{i=1}^{n}b_i=1 \\}\n",
    "\\end{align}\n",
    "\n",
    "Better approach: Transform into an optmization problem like\n",
    "\n",
    "\\begin{align}\n",
    "    x^* = argmin f\\left(x;b\\right)\\\\\n",
    "    u.c \\text{ } \\mathbb{1}'x = 1, \\mathbb{0} \\leq x \\leq \\mathbb{1}\n",
    "\\end{align}\n",
    "\n",
    "As an example, $f$ could be specified as\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    f\\left(x;b\\right) = \\sum_{i=1}^{n}\\left(x_i \\partial \\mathcal{R}\\left(x\\right) - b_i \\mathcal{R}\\left(x\\right)\\right)^2\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Which can be solved by the Sequential Quadratic Programing Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.ones(len(selected_securities))/ np.sum(np.ones(len(selected_securities)))\n",
    "\n",
    "plt.figure(figsize=(30,12))\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(b, labels = selected_securities)\n",
    "ax1.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocation example\n",
    "# Settings\n",
    "from RiskBudgeting import RiskBudgeting, plot_pnl_evolution, compute_pnl_evolution\n",
    "from scipy import optimize\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "\n",
    "eps = 0.000000000000001\n",
    "large_number = 10000000\n",
    "risk_target = 0.25\n",
    "ann_factor = 252\n",
    "\n",
    "b = np.ones(len(selected_securities))/ np.sum(np.ones(len(selected_securities)))\n",
    "\n",
    "# Functions\n",
    "risk_obj = RiskBudgeting()\n",
    "\n",
    "sigma2_copula = np.exp(scenario_projections_copula - 1).cov().to_numpy()\n",
    "sigma2_ld =  np.exp(scenario_projections_ld - 1).cov().to_numpy()\n",
    "\n",
    "# Risk Measure\n",
    "PRC_copula = lambda w: risk_obj.risk_contribution(w, sigma2_copula) / np.sum(risk_obj.risk_contribution(w, sigma2_copula))\n",
    "PRC_ld = lambda w: risk_obj.risk_contribution(w, sigma2_ld) / np.sum(risk_obj.risk_contribution(w, sigma2_ld))\n",
    "\n",
    "# Defining problem initial values, constraints and boundaries\n",
    "c = b.dot(np.log(b)) - eps\n",
    "\n",
    "def ineq_constraint(x, b_target, c):\n",
    "    return np.transpose(b_target).dot(np.log(x)) - c\n",
    "\n",
    "# w0 = np.ones(b.shape)*1\n",
    "bounds = [(eps, np.inf)] * b.shape[0]\n",
    "\n",
    "ineq_const = {'type': 'ineq',\n",
    "              'fun': lambda x: ineq_constraint(x, b, c),\n",
    "              'jac': lambda x: b * (1 / x)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing - Ledoit and Wolf\n",
    "flag = True\n",
    "while flag:\n",
    "    shape_gamma = np.random.uniform(0.01, 5, 1)\n",
    "    scale_gamma = np.random.uniform(0.01, 10, 1)\n",
    "    list_w0 = np.random.gamma(shape=shape_gamma[0], scale=scale_gamma[0], size=b.shape[0])\n",
    "\n",
    "    res_ld = optimize.minimize(lambda x: risk_obj.risk_measure(x, sigma2_ld),\n",
    "                                   x0=list_w0,\n",
    "                                   method='SLSQP',\n",
    "                                   constraints=ineq_const,\n",
    "                                   bounds=bounds,\n",
    "                                   jac=lambda x: risk_obj.jacobian(x, sigma2_ld),\n",
    "                                   options={'maxiter': 10000000, 'ftol': 1e-20})\n",
    "\n",
    "    if res_ld.status == 0:\n",
    "        flag = False\n",
    "\n",
    "print(res_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing - Copula - without VaR restriction\n",
    "flag = True\n",
    "while flag:\n",
    "    shape_gamma = np.random.uniform(0.01, 5, 1)\n",
    "    scale_gamma = np.random.uniform(0.01, 10, 1)\n",
    "    list_w0 = np.random.gamma(shape=shape_gamma[0], scale=scale_gamma[0], size=b.shape[0])\n",
    "\n",
    "    res_copula = optimize.minimize(lambda x: risk_obj.risk_measure(x, sigma2_copula),\n",
    "                                   x0=list_w0,\n",
    "                                   method='SLSQP',\n",
    "                                   constraints=ineq_const,\n",
    "                                   bounds=bounds,\n",
    "                                   jac=lambda x: risk_obj.jacobian(x, sigma2_copula),\n",
    "                                   options={'maxiter': 10000000, 'ftol': 1e-20})\n",
    "\n",
    "    if res_copula.status == 0:\n",
    "        flag = False\n",
    "\n",
    "print(res_copula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing - Copula - with V@R Restriction\n",
    "confidence = 0.99\n",
    "var_at_confidence = -0.5\n",
    "\n",
    "def value_at_risk(x, scenarios, confidence, norm_func=None):\n",
    "    if norm_func is not None:\n",
    "        factor = norm_func(x)\n",
    "    else:\n",
    "        factor = 1\n",
    "\n",
    "    loss = -1 * np.sum((x * factor) * scenarios, axis=1).sort_values(ascending=True)\n",
    "    confidence_vector = np.ones(scenarios.shape[0])\n",
    "    confidence_vector = 1 - np.cumsum(confidence_vector / np.sum(confidence_vector))\n",
    "    flag = (confidence_vector <= confidence)\n",
    "    return -loss[flag].values[0]\n",
    "\n",
    "def fun_con(x):\n",
    "    return value_at_risk(x, np.exp(scenario_projections_copula - 1), confidence, norm_func = lambda y: risk_target / risk_obj.risk_measure(y, sigma2_copula)[0]) - var_at_confidence\n",
    "\n",
    "constrains = [ineq_const, NonlinearConstraint(fun=fun_con, lb=0, ub=np.inf)]\n",
    "\n",
    "flag = True\n",
    "while flag:\n",
    "    shape_gamma = np.random.uniform(0.01, 5, 1)\n",
    "    scale_gamma = np.random.uniform(0.01, 10, 1)\n",
    "    list_w0 = np.random.gamma(shape=shape_gamma[0], scale=scale_gamma[0], size=b.shape[0])\n",
    "\n",
    "    res_copula_w_restriction = optimize.minimize(lambda x: risk_obj.risk_measure(x, sigma2_copula),\n",
    "                                                  x0=list_w0,\n",
    "                                                  method='SLSQP',\n",
    "                                                  constraints=constrains,\n",
    "                                                  bounds=bounds,\n",
    "                                                  jac=lambda x: risk_obj.jacobian(x, sigma2_copula),\n",
    "                                                  options={'maxiter': 10000000, 'ftol': 1e-20})\n",
    "\n",
    "    if res_copula_w_restriction.status == 0:\n",
    "        flag = False\n",
    "\n",
    "print(res_copula_w_restriction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocation Weights\n",
    "factor_copula = risk_target / risk_obj.risk_measure(res_copula.x, sigma2_copula)[0]\n",
    "factor_copula_w_restriction = risk_target / risk_obj.risk_measure(res_copula_w_restriction.x, sigma2_copula)[0]\n",
    "factor_ld = risk_target / risk_obj.risk_measure(res_ld.x, sigma2_ld)[0]\n",
    "\n",
    "print(\"Allocation Weights\\n\")\n",
    "allocation_weights = pd.DataFrame({\"VineCopula\":res_copula.x * factor_copula,\n",
    "                                   \"VineCopulaV@R\":res_copula_w_restriction.x * factor_copula_w_restriction,\n",
    "                                   \"LedoitWolf\":res_ld.x * factor_ld}, index=selected_securities)\n",
    "print(allocation_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Accumulated Returns - Backtest\n",
    "print(\"Backtests\")\n",
    "quotes_os = pd.DataFrame({\"VineCopula\": compute_pnl_evolution(allocation_weights[\"VineCopula\"], df_log_ret.iloc[-n_os:, :] / returns_scale_factor), \"VineCopulaV@R\": compute_pnl_evolution(allocation_weights[\"VineCopulaV@R\"], df_log_ret.iloc[-n_os:, :]/returns_scale_factor), \"Ledoit\": compute_pnl_evolution(allocation_weights[\"LedoitWolf\"], df_log_ret.iloc[-n_os:, :]/returns_scale_factor)}, index=df_log_ret.iloc[-n_os:, :].index)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(quotes_os.iloc[:, 0], label=\"Vine\")\n",
    "ax.plot(quotes_os.iloc[:, 1], label=\"VineV@R\")\n",
    "ax.plot(quotes_os.iloc[:, 2], label=\"Ledoit\")\n",
    "leg = ax.legend()\n",
    "fig.set_size_inches(20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest Realized Volatility - Out of Sample\n",
    "print(np.log(quotes_os).diff().std() * np.sqrt(252))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting P&L Copula Scenarios - Together with the Backtest\n",
    "fig, ax = plot_pnl_evolution(allocation_weights[\"VineCopula\"], [item['projections']/returns_scale_factor for item in scenarios_copula], set_size_inches = [15, 15])\n",
    "ax.set_ylim(-1, 2)\n",
    "\n",
    "ax.plot(quotes_os.iloc[:, 1], color=\"green\", linewidth=3)\n",
    "ax.set_ylim(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting P&L Ledoit and Wolf Scenarios - Together with the Backtest\n",
    "fig, ax = plot_pnl_evolution(allocation_weights[\"VineCopula\"], [item['projections']/returns_scale_factor for item in scenarios_ld], set_size_inches = [15, 15])\n",
    "ax.set_ylim(-1, 2)\n",
    "\n",
    "ax.plot(quotes_os.iloc[:, 1], color=\"green\", linewidth=3)\n",
    "ax.set_ylim(-1, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
